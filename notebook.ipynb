{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Voice-to-Text Extraction of Nepali Recordings with Multiple Agent Detection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Link**: https://www.kaggle.com/datasets/ishworsubedii/nepali-speech-to-text-dataset\n",
    "\n",
    "- Used 10 audio files from this dataset for quick demonstration. (2079-11-21_1.wav to 2079-11-21_10.wav)\n",
    "- Download the dataset and extract it inside a new folder called \"Dataset\" in work working directory (Same directory as notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyxlsb import open_workbook\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "from pyannote.audio.pipelines.speaker_diarization import SpeakerDiarization\n",
    "from pyannote.core import Segment\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation:**\n",
    "\n",
    "This section loads the Nepali speech transcript dataset from an Excel Binary (.xlsb) file. The code extracts audio paths and corresponding transcriptions, converting them to a pandas DataFrame and saving as CSV for easier access in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Audio Path  \\\n",
      "0  Nepali Speech To Text Dataset\\audio_chunks\\207...   \n",
      "1  Nepali Speech To Text Dataset\\audio_chunks\\207...   \n",
      "2  Nepali Speech To Text Dataset\\audio_chunks\\207...   \n",
      "3  Nepali Speech To Text Dataset\\audio_chunks\\207...   \n",
      "4  Nepali Speech To Text Dataset\\audio_chunks\\207...   \n",
      "\n",
      "                                       Transcription  \n",
      "0  प्रसाद विश्वकर्मा  सम्माननीय अध्यक्षमहोदय, आजक...  \n",
      "1  अभ्यास गर्ने कुराको विषयमा त्यसमा उल्लेख छ । र...  \n",
      "2   खम्बा बिद्रोह भएको कुरा चाहिँ हामीलाई थाहा छ ...  \n",
      "3  अन्त्य भई शान्ति प्रक्रिया सुरु भएको सोह्र वर्...  \n",
      "4  म जस्तै अरुलाई सम्झन्छु र चित्त बुझाउँछु । भाव...  \n"
     ]
    }
   ],
   "source": [
    "# Read .xlsb file\n",
    "xlsb_file = \"Dataset/Nepali Speech To Text Dataset/transcripts/audio transcript.xlsb\"  \n",
    "\n",
    "data = []\n",
    "with open_workbook(xlsb_file) as wb:\n",
    "    with wb.get_sheet(1) as sheet:\n",
    "        for row in sheet.rows():\n",
    "            values = [item.v for item in row]\n",
    "            data.append(values)\n",
    "\n",
    "# Skip the header row\n",
    "data = data[1:]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Audio Path\", \"Transcription\"])\n",
    "df.to_csv(\"transcripts.csv\", index=False)\n",
    "\n",
    "print(df.head())  # Verify structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio Preprocessing**\n",
    "\n",
    "The preprocessing code converts audio files to a standardized format appropriate for speech recognition models. Each WAV file is resampled to 16kHz and converted to mono channel (1-channel), which are standard requirements for many ASR models. This preprocessing ensures consistent audio quality for the downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio processing complete!\n"
     ]
    }
   ],
   "source": [
    "audio_dir = \"Dataset/Nepali Speech To Text Dataset/audio_chunks\"\n",
    "output_dir = \"processed_audio/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(audio_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio = AudioSegment.from_wav(os.path.join(audio_dir, file))\n",
    "        audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "        audio.export(os.path.join(output_dir, file), format=\"wav\")\n",
    "\n",
    "print(\"Audio processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**  (Test Pre-trained Models)\n",
    "\n",
    "Initial Whisper Test\n",
    "\n",
    "This code block tests the Whisper large model on a single Nepali audio file. The large model was selected because it demonstrated superior performance on Nepali language compared to small and medium variants, leading to more accurate transcriptions of specialized language and dialects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " বেবববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববব�ববববববববববব focal��ববববববববববববববব\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"medium\") \n",
    "audio_path = \"processed_audio/2079-11-21_1.wav\"\n",
    "\n",
    "result = model.transcribe(audio_path, language=\"ne\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " प्रसाद विश्वकर्मा सम्माननी अदक्षि मोदे आज को नागरिक दैनिग मा योटा समाचार चेने आया को छा नेपाली सेना रो अम्रिकी सेना को संयुक्त तालिम राज्या\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"large\") \n",
    "audio_path = \"processed_audio/2079-11-21_1.wav\"\n",
    "\n",
    "result = model.transcribe(audio_path, language=\"ne\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " अजा ख़ाद बिस्चवा कर माः सम्मानु नि अदक्सि मोडे अजा कु नगरिक देनिग मा योडा समचार चेने आगो जा नेपाली सेना रामरिकी सेना को संविड्टा पाली मरगरिक\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"small\") \n",
    "audio_path = \"processed_audio/2079-11-21_1.wav\"\n",
    "\n",
    "result = model.transcribe(audio_path, language=\"ne\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Transcription for All Files with Whisper**\n",
    "\n",
    "This section processes all preprocessed audio files through the Whisper large model, specifically instructing it to use Nepali language (\"ne\") for transcription. The results are saved to a CSV file with filenames and their corresponding transcriptions, creating a complete dataset of machine-generated transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete! Check whisper_transcriptions.csv\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper Large model\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "audio_dir = \"processed_audio/\"\n",
    "transcriptions = []\n",
    "\n",
    "for file in os.listdir(audio_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_dir, file)\n",
    "        result = model.transcribe(audio_path, language=\"ne\")\n",
    "        transcriptions.append([file, result[\"text\"]])\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(transcriptions, columns=[\"Audio File\", \"Transcription\"])\n",
    "df.to_csv(\"whisper_transcriptions.csv\", index=False)\n",
    "\n",
    "print(\"Transcription complete! Check whisper_transcriptions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speaker Identification (Diarization)**\n",
    "\n",
    "The diarization code implements speaker identification using the pyannote.audio library. It processes each audio file to identify different speakers and their time segments, enabling the distinction between different agents in the customer service calls. The authentication with Hugging Face token ensures access to the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aniket Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1004: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Aniket Singh\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\Aniket Singh\\Desktop\\ASR_Nepal\\.venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2079-11-21_1.wav Speaker Segments:\n",
      "SPEAKER_01: 0.03 sec - 0.99 sec\n",
      "SPEAKER_00: 13.35 sec - 15.05 sec\n",
      "SPEAKER_02: 19.02 sec - 19.54 sec\n",
      "SPEAKER_01: 19.54 sec - 24.36 sec\n",
      "SPEAKER_01: 26.22 sec - 30.47 sec\n",
      " 2079-11-21_10.wav Speaker Segments:\n",
      "SPEAKER_01: 0.03 sec - 11.54 sec\n",
      "SPEAKER_01: 12.37 sec - 12.99 sec\n",
      "SPEAKER_00: 13.97 sec - 26.52 sec\n",
      "SPEAKER_00: 27.77 sec - 30.47 sec\n",
      " 2079-11-21_2.wav Speaker Segments:\n",
      "SPEAKER_00: 0.03 sec - 30.47 sec\n",
      " 2079-11-21_3.wav Speaker Segments:\n",
      "SPEAKER_01: 0.03 sec - 17.07 sec\n",
      "SPEAKER_02: 17.07 sec - 18.02 sec\n",
      "SPEAKER_02: 19.49 sec - 20.84 sec\n",
      "SPEAKER_00: 26.36 sec - 30.47 sec\n",
      " 2079-11-21_4.wav Speaker Segments:\n",
      "SPEAKER_00: 0.03 sec - 30.09 sec\n",
      "SPEAKER_01: 28.82 sec - 29.58 sec\n",
      " 2079-11-21_5.wav Speaker Segments:\n",
      "SPEAKER_01: 0.03 sec - 3.02 sec\n",
      "SPEAKER_01: 3.83 sec - 24.92 sec\n",
      "SPEAKER_00: 3.96 sec - 4.91 sec\n",
      "SPEAKER_00: 25.48 sec - 27.18 sec\n",
      " 2079-11-21_6.wav Speaker Segments:\n",
      "SPEAKER_00: 3.63 sec - 20.40 sec\n",
      "SPEAKER_00: 20.99 sec - 30.07 sec\n",
      " 2079-11-21_7.wav Speaker Segments:\n",
      "SPEAKER_02: 0.03 sec - 12.05 sec\n",
      "SPEAKER_01: 12.97 sec - 15.27 sec\n",
      "SPEAKER_00: 19.81 sec - 21.21 sec\n",
      "SPEAKER_00: 22.49 sec - 30.47 sec\n",
      " 2079-11-21_8.wav Speaker Segments:\n",
      "SPEAKER_00: 0.03 sec - 30.47 sec\n",
      "SPEAKER_01: 24.42 sec - 26.25 sec\n",
      " 2079-11-21_9.wav Speaker Segments:\n",
      "SPEAKER_01: 0.03 sec - 6.02 sec\n",
      "SPEAKER_02: 6.38 sec - 7.84 sec\n",
      "SPEAKER_00: 12.77 sec - 16.96 sec\n",
      "SPEAKER_00: 17.92 sec - 30.47 sec\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Hugging Face authentication token from the environment variables\n",
    "auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "\n",
    "# Load Pyannote Pretrained Model with authentication\n",
    "diarization_model = SpeakerDiarization.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=auth_token)\n",
    "\n",
    "# Assuming you have an 'audio_dir' variable defined\n",
    "audio_dir = \"processed_audio/\"\n",
    "\n",
    "# Process Each Audio File\n",
    "for file in os.listdir(audio_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_dir, file)\n",
    "        \n",
    "        # Perform diarization\n",
    "        diarization = diarization_model(audio_path)\n",
    "\n",
    "        print(f\" {file} Speaker Segments:\")\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            print(f\"{speaker}: {turn.start:.2f} sec - {turn.end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "This final section evaluates the performance of the Whisper model by calculating Word Error Rate (WER) between the ground truth transcriptions and the model's output. The relatively high WER observed (which would be shown in the notebook output) can be attributed to several factors:\n",
    "\n",
    "- Small dataset size (only 10 audio files)\n",
    "\n",
    "- Complexity of Nepali language and dialects\n",
    "\n",
    "- Potential differences in transcription style between ground truth and model output\n",
    "\n",
    "- Limitations in ASR models for low-resource languages like Nepali\n",
    "\n",
    "- Possible presence of domain-specific terminology in customer service conversations\n",
    "\n",
    "- The evaluation provides a quantitative measure of model performance while acknowledging the challenging nature of the task and dataset limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 1.04\n"
     ]
    }
   ],
   "source": [
    "# Load Ground Truth and Predictions\n",
    "df = pd.read_csv(\"whisper_transcriptions.csv\")  # Transcribed texts\n",
    "ground_truth = pd.read_csv(\"transcripts.csv\")  # Original transcriptions\n",
    "\n",
    "# Ensure correct matching\n",
    "df = df.sort_values(\"Audio File\")\n",
    "ground_truth = ground_truth.sort_values(\"Audio Path\")\n",
    "\n",
    "# Calculate WER for each audio\n",
    "wer_scores = []\n",
    "for i in range(len(df)):\n",
    "    pred_text = df.iloc[i][\"Transcription\"]\n",
    "    true_text = ground_truth.iloc[i][\"Transcription\"]\n",
    "\n",
    "    wer_score = wer(true_text, pred_text)\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "# Average WER\n",
    "avg_wer = sum(wer_scores) / len(wer_scores)\n",
    "print(f\"Average WER: {avg_wer:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
